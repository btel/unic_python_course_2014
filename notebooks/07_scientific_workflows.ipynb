{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific workflows\n",
    "\n",
    "The examples for the tutorial are in `materials/07_scientific_workflows` directory.\n",
    "\n",
    "To run the examples change to this directory and run either `doit` (you will need to install the [doit](http://pydoit.org) library) or `sh run_pipeline.sh`\n",
    "\n",
    "## Example:\n",
    "\n",
    "â€¢ distribution of pairwise correlations\n",
    "\n",
    "## Outline\n",
    "\n",
    "* what is a scientific workflow\n",
    "* points to remember:\n",
    "   - programming in general is an iterative process\n",
    "   - keep it simple stupid\n",
    "   - repetition is the root of all evil\n",
    "   - program to interfaces\n",
    "* three components of scientific workflow:\n",
    "   - data generation (simulation, pre-processing)\n",
    "   - statistics\n",
    "   - plots\n",
    "* two stages of data analysis:\n",
    "   - exploratory on a single (or a few) datasets\n",
    "   - batch analysis\n",
    "* writing simple command line tools\n",
    "   - argparse\n",
    "   - avoid changes in API\n",
    "* saving data to files:\n",
    "   - csv\n",
    "   - compare pickle and npz\n",
    "* folder structure\n",
    "   - separate folders for figures, results, data\n",
    "* writing meta-scripts in bash/python\n",
    "* running batch analyses\n",
    "* using automatic builders (doit)\n",
    "\n",
    "# Plan:\n",
    "\n",
    "1) Single cell analysis\n",
    "\n",
    "   * write simple script to read data, calculate correlation coefficients and plot their histogram\n",
    "   * use argparse to specify the input filename from the command line\n",
    "\n",
    "2) Building first workflow\n",
    "\n",
    "   * make separate folders for figures and results\n",
    "   * allow to write correlation coefficients to file\n",
    "   * seperate plotting and analysis\n",
    "\n",
    "2) Batch processing\n",
    "\n",
    "   * write data merge script\n",
    "   * write python script using subprocess, which run the correlation analysis on all its input files\n",
    "   * allow to plot histograms from more than one file\n",
    "   * write bash script to run the entire pipeline\n",
    "\n",
    "3) Automating\n",
    "   \n",
    "   * write `dodo.py` which runs the batch analysis on V1 data - define input files by hand\n",
    "   * add an extra input file to dependency list and show that the task will be automatically re-executed\n",
    "   * add plotting task based on v1 data only\n",
    "   * implement partially the batch run in the dodo file\n",
    "\n",
    "# Questions and answers:\n",
    "\n",
    "* In batch analysis can I import function rather than call external python scripts with `subprocess`?\n",
    "\n",
    "  Both solutions are possible. The advantage of calling the external scripts is that you can also combine programs written in other languages (like C, Matlab, Java). In addition, it requires only few changes to your exisiting script for exploratory data analysis. In the script you can use all the power of python including the global variables and you do not have to clean up your code after each call (no spill of data between independent runs of the analysis).\n",
    "  \n",
    "  Importing and calling a function in the batch script can potentially save some time required to initialize external libraries etc.\n",
    "  \n",
    "  In some cases it is not possible to clean up the interpreter after a run (for example, for simulations involving neuron simulator). In this case one needs to resort to first mechanism.  \n",
    "  \n",
    "  \n",
    "* Will this approach produce too many script files?\n",
    "\n",
    "  Yes, this is a danger. One should make sure that scripts are not too simple, but on the other hand are re-useable. Remember nothing will replace your common sense. The workflow I presented is just a set of recipes that I found useful in my work. You can pick and mix the ones that you find useful.\n",
    "  \n",
    "  There are also other approaches for building workflows each with its own pros and cons:\n",
    "  \n",
    "    * `luigi`\n",
    "    * `joblib`\n",
    "    * `mdp` -- modular data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
